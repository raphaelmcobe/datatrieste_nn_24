<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.131.0">
    <meta charset="utf-8">
<title>Introduction to Artificial Neural Networks</title>
<meta name="description" content="Intro to ANN Presentation">
<meta name="author" content="Raphael Cobe">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="./reveal-js/css/reset.css">
<link rel="stylesheet" href="./reveal-js/css/reveal.css"><link rel="stylesheet" href="./reveal-hugo/themes/robot-lung.css" id="theme"><link rel="stylesheet" href="./highlight-js/mono-blue.min.css">
    

<style>
 
.reveal section pre {
  box-shadow: none;
  margin-top: 25px;
  margin-bottom: 25px;
  border: 1px solid lightgrey;
}
.reveal section pre:hover {
  border: 1px solid grey;
  transition: border 0.3s ease;
}
.reveal section pre > code {
  padding: 10px;
}
.reveal table {
  font-size: 0.65em;
}
 
.reveal section.side-by-side h1 {
  position: absolute;
}
.reveal section.side-by-side h1:first-of-type {
  left: 25%;
}
.reveal section.side-by-side h1:nth-of-type(2) {
  right: 25%;
}
.reveal section[data-background-image] a,
.reveal section[data-background-image] p,
.reveal section[data-background-image] h2 {
  color: white;
}
.reveal section[data-background-image] a {
  text-decoration: underline;
}
</style>

  </head>
  <body>
    
    <style>
      #logo {
        position: absolute;
        top: 1%;
        left: 1%;
        width: 15%;
      }
    </style>
    <img id="logo" src="logo_ai2.png" alt="Advanced Institute for Artificial Intelligence">
    
    <div class="reveal">
      <div class="slides">
  

    <section><h1 id="introduction-to-neural-networks">Introduction to Neural Networks</h1>
<p>~ by <a href="mailto:raphael.cobe@advancedinstitute.ai">@raphaelmcobe</a> ~</p>
</section>

  

    <section><h2 id="neural-networks">Neural Networks</h2>
<ul>
<li>Neurons as structural constituents of the brain <a href="http://hobertlab.org/wp-content/uploads/2014/10/Andres-Barquin_Cajal_2001.pdf" target="_blank">[Ramón y Cajál, 1911]</a>;</li>
<li>Five to six orders of magnitude <em>slower than silicon logic gates</em>;</li>
<li>In a silicon chip happen in the <em>nanosecond (on chip)</em> vs <em>millisecond range (neural events)</em>;</li>
<li>A truly staggering number of neurons (nerve cells) with <em>massive interconnections between them</em>;</li>
</ul>
</section><section>
<h2 id="neural-networks-1">Neural Networks</h2>
<ul>
<li>Receive input from other units and decides whether or not to fire;</li>
<li>Approximately <em>10 billion neurons</em> in the human cortex, and <em>60 trillion synapses</em> or connections <a href="https://www.researchgate.net/publication/37597256_Biophysics_of_Computation_Neurons_Synapses_and_Membranes" target="_blank">[Shepherd and Koch, 1990]</a>;</li>
<li>Energy efficiency of the brain is approximately $10^{−16}$ joules per operation per second against ~ $10^{−8}$ in a computer;</li>
</ul>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="neuron2.png">
  
<h2 id="neurons">Neurons</h2>
</section><section>
<h2 id="neurons-1">Neurons</h2>
<ul>
<li>input signals from its <em>dendrites</em>;</li>
<li>output signals along its (single) <em>axon</em>;</li>
</ul>
<img src="neuron1.png"/>



<aside class="notes"><ul>
<li>three major types of neurons: <em>sensory neurons</em>, <em>motor neurons</em>, and <em>interneurons</em></li>
</ul>
</aside>
</section><section>
<h2 id="neurons-2">Neurons</h2>
<h3 id="how-do-they-work">How do they work?</h3>
<ul>
<div align="left">


<span class='fragment ' > <li>Control the influence from one neuron on another:</li> </span>


</div>
<ul>
<div align="left">


<span class='fragment ' > <li><em>Excitatory</em> when weight is positive; or</li> </span>


</div>
<div align="left">


<span class='fragment ' > <li><em>Inhibitory</em> when weight is negative;</li> </span>


</div>
</ul>
<div align="left">


<span class='fragment ' > <li>Nucleus is responsible for summing the incoming signals;</li> </span>


</div>
<div align="left">


<span class='fragment ' > <li><strong>If the sum is above some threshold, then <em>fire!</em></strong></li> </span>


</div>
</ul>
</section><section>
<h2 id="neurons-3">Neurons</h2>
<h3 id="artificial-neuron">Artificial Neuron</h3>
<center><img src="artificial_neuron.jpeg" width="800px"/></center>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="neurons.png">
  
<h2 id="neural-networks-2">Neural Networks</h2>
</section><section>
<h2 id="neural-networks-3">Neural Networks</h2>
<ul>
<li>It appears that one reason why the human brain is <em>so powerful</em> is the
sheer complexity of connections between neurons;</li>
<li>The brain exhibits <em>huge degree of parallelism</em>;</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks">Artificial Neural Networks</h2>
<ul>
<li>Model each part of the neuron and interactions;</li>
<li><em>Interact multiplicatively</em> (e.g. $w_0x_0$) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. $w_0$ );</li>
<li>Learn <em>synapses strengths</em>;</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-1">Artificial Neural Networks</h2>
<h3 id="function-approximation-machines">Function Approximation Machines</h3>
<ul>
<li>Datasets as composite functions: $y=f^{*}(x)$
<ul>
<li>Maps $x$ input to a category (or a value) $y$;</li>
</ul>
</li>
<li>Learn synapses weights and aproximate $y$ with $\hat{y}$:
<ul>
<li>$\hat{y} = f(x;w)$</li>
<li>Learn the $w$ parameters;</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-2">Artificial Neural Networks</h2>
<ul>
<li>Can be seen as a directed graph with units (or neurons) situated at the vertices;
<ul>
<li>Some are <em>input units</em></li>
</ul>
</li>
<li>Receive signal from the outside world;</li>
<li>The remaining are named <em>computation units</em>;</li>
<li>Each unit <em>produces an output</em>
<ul>
<li>Transmitted to other units along the arcs of the directed graph;</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-3">Artificial Neural Networks</h2>
<ul>
<li><em>Input</em>, <em>Output</em>, and <em>Hidden</em> layers;</li>
<li>Hidden as in &ldquo;not defined by the output&rdquo;;</li>
</ul>
<center><img src="nn1.png" height="200px" style="margin-top:50px;"/></center>
</section><section>
<h2 id="artificial-neural-networks-4">Artificial Neural Networks</h2>
<h6 id="motivation-example-taken-from-jay-alammar-a-hrefhttpsjalammargithubiovisual-interactive-guide-basics-neural-networks-target_blankblog-posta">Motivation Example (taken from Jay Alammar <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">blog post</a>)</h6>
<ul>
<li>Imagine that you want to forecast the price of houses at your neighborhood;
<ul>
<li>After some research you found that 3 people sold houses for the following values:</li>
</ul>
</li>
</ul>
<br />
<table>
<thead>
<tr>
<th>Area (sq ft) (x)</th>
<th>Price (y) in USD</th>
</tr>
</thead>
<tbody>
<tr>
<td>2,104</td>
<td>$399,900$</td>
</tr>
<tr>
<td>1,600</td>
<td>$329,900$</td>
</tr>
<tr>
<td>2,400</td>
<td>$369,000$</td>
</tr>
</tbody>
</table>
</section><section>
<h2 id="artificial-neural-networks-5">Artificial Neural Networks</h2>
<h6 id="motivation-example-taken-from-jay-alammar-a-hrefhttpsjalammargithubiovisual-interactive-guide-basics-neural-networks-target_blankblog-posta-1">Motivation Example (taken from Jay Alammar <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">blog post</a>)</h6>
<p>

<span class='fragment ' >If you want to sell a 2K sq ft house, how much should ask for it?</span>


<br /><br />


<span class='fragment ' >How about finding the <em>average price per square feet</em>?</span>


<br /><br />


<span class='fragment ' ><em>$180 per sq ft.</em></span>

</p>
</section><section>
<h2 id="artificial-neural-networks-6">Artificial Neural Networks</h2>
<h6 id="motivation-example-taken-from-jay-alammar-a-hrefhttpsjalammargithubiovisual-interactive-guide-basics-neural-networks-target_blankblog-posta-2">Motivation Example (taken from Jay Alammar <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">blog post</a>)</h6>
<ul>
<li>Our very first neural network looks like this:


<span class='fragment ' ><center><img src="nn2.png" width="600px"/></center> </span>

</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-7">Artificial Neural Networks</h2>
<h6 id="motivation-example-taken-from-jay-alammar-a-hrefhttpsjalammargithubiovisual-interactive-guide-basics-neural-networks-target_blankblog-posta-3">Motivation Example (taken from Jay Alammar <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">blog post</a>)</h6>
<ul>
<li>Multiplying $2,000$ sq ft by $180$ gives us  $360,000$.</li>
<li>Calculating the prediction is simple multiplication.</li>
<li><strong><em>We needed to think about the weight we’ll be multiplying by.</em></strong></li>
<li>That is what training means!</li>
</ul>
<br />
<table>
<thead>
<tr>
<th>Area (sq ft) (x)</th>
<th>Price (y)</th>
<th>Estimated Price($\hat{y}$)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2,104</td>
<td>$399,900$</td>
<td>$378,720$</td>
</tr>
<tr>
<td>1,600</td>
<td>$329,900$</td>
<td>$288,000$</td>
</tr>
<tr>
<td>2,400</td>
<td>$369,000$</td>
<td>$432,000$</td>
</tr>
</tbody>
</table>
</section><section>
<h2 id="artificial-neural-networks-8">Artificial Neural Networks</h2>
<h6 id="motivation-example-taken-from-jay-alammar-a-hrefhttpsjalammargithubiovisual-interactive-guide-basics-neural-networks-target_blankblog-posta-4">Motivation Example (taken from Jay Alammar <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">blog post</a>)</h6>
<ul>
<li>How bad is our model?
<ul>
<li>Calculate the <em>Error</em>;</li>
<li>A better model is one that has less error;</li>
</ul>
</li>
</ul>
<p>

<span class='fragment ' ><em>Mean Square Error</em></span>



<span class='fragment ' >: $2,058$</span>

</p>
<br />
<table>
<thead>
<tr>
<th>Area (sq ft) (x)</th>
<th>Price (y)</th>
<th>Estimated Price($\hat{y}$)</th>
<th>$y-\hat{y}$</th>
<th>$(y-\hat{y})^2$</th>
</tr>
</thead>
<tbody>
<tr>
<td>2,104</td>
<td>$399,900$</td>
<td>$378,720$</td>
<td>$21$</td>
<td>$449$</td>
</tr>
<tr>
<td>1,600</td>
<td>$329,900$</td>
<td>$288,000$</td>
<td>$42$</td>
<td>$1756$</td>
</tr>
<tr>
<td>2,400</td>
<td>$369,000$</td>
<td>$432,000$</td>
<td>$-63$</td>
<td>$3969$</td>
</tr>
</tbody>
</table>
</section><section>
<h2 id="artificial-neural-networks-9">Artificial Neural Networks</h2>
<ul>
<li>Fitting the line to our data:</li>
</ul>
<center><img src="manual_training1.gif" width="450px"/></center>
<p>Follows the equation: $\hat{y} = W * x$</p>
</section><section>
<h2 id="artificial-neural-networks-10">Artificial Neural Networks</h2>
<p>How about addind the <em>Intercept</em>?</p>


<span class='fragment ' >$\hat{y}=Wx + b$</span>


</section><section>
<h2 id="artificial-neural-networks-11">Artificial Neural Networks</h2>
<h3 id="the-bias">The Bias</h3>
<center><img src="nn3.png" width="500px"/></center>
</section><section>
<h2 id="artificial-neural-networks-12">Artificial Neural Networks</h2>
<h3 id="try-to-train-it-manually">Try to train it manually:</h3>
<iframe src="manual_NN1.html" height="500px" width="800px">
</iframe>
</section><section>
<h2 id="residuals">Residuals</h2>
<img src="residuos_ilustracao.png" width="600px">
</section><section>
<ul>
<li>One way to calculate $w_0$ and $w_1$ is based on the sum of squared residuals (RSS - <em>R</em>esidual <em>S</em>um of <em>S</em>quares):</li>
</ul>
<p>$J(W) = \frac{1}{2n}\sum_{i=1}^{n} \epsilon_i^2$</p>
</section><section>
<h2 id="variation-of-the-cost-function">Variation of the Cost Function</h2>
<h3 id="examining-epsilon-as-a-function-of-w_0-and-w_1">Examining $\epsilon$ as a function of $w_0$ and $w_1$</h3>
<p><img src="funcao_custo.png" alt="Cost Function"></p>
</section><section>
<h2 id="finding-values-of-w_0-and-w_1">Finding values of $w_0$ and $w_1$</h2>
<h3 id="gradient-calculation">Gradient Calculation</h3>
<ul>
<li>The gradient of a vector is a generalization of the derivative and is represented by the vector operator $\nabla$. This operation is used to minimize our cost function (<em>RSS</em>):</li>
<li>Method of <em>Ordinary Least Squares</em> (OLS)</li>
</ul>
</section><section>
<h2 id="finding-values-of-w_0-and-w_1-1">Finding values of $w_0 and $w_1$</h2>
<h3 id="the-problem-with-the-analytical-solution">The problem with the analytical solution</h3>
<ul>
<li>The analytical solution in its vector form is:</li>
</ul>
<p>$W = (X^T X)^{-1} X^T Y$</p>
<ul>
<li>Disadvantages of using the analytical solution:
<ul>
<li>$X^T X$ is not always invertible;</li>
<li>The complexity of computing the inverse is of the order $O(n^3)$:
<ul>
<li>If the number of features is high, it can become <em>computationally expensive</em>;</li>
<li>Extremely high memory consumption.</li>
</ul>
</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="finding-values-of-w_0-and-w_1-2">Finding values of $w_0$ and $w_1$</h2>
<h3 id="inverse-calculation">Inverse Calculation</h3>
<font size="6">
<em>Problem</em>
</font>
<p>Imagine a dataset containing $10^5$ features and $10^6$ observations; in this case, $X^T X$ would have $10^5 \times 10^5$ floating points which, at 8 bytes per number, would require <em>80 gigabytes</em>. The inverse calculation would then consume on the order of $O(n^3)$.</p>


<span class='fragment ' ><em>80 kilo-yottabytes!!!</em></span>


</section><section>
<h3 id="heading"></h3>
<font size="5">
<p align="justify">
Usually, in classification problems, we have the impression that the more features we have, the better our technique's accuracy will be. However, in real-world problems where we have a <em>limited number of samples</em>, a phenomenon known as the <em>curse of dimensionality</em> is observed. There are several negative effects caused by the indiscriminate increase in features:
</p>
</font>


<span class='fragment ' ><ul>
<li><strong>Hughes Phenomenon:</strong> For a finite number of samples, there exists a dimensionality $d^\ast$ beyond which the classification accuracy decreases.</li>
</ul>
</span>




<span class='fragment ' ><img src="PCA_Fig2.png" width="300px">
</span>


</section><section>
<h2 id="artificial-neural-networks-13">Artificial Neural Networks</h2>
<h3 id="how-to-discover-the-correct-weights">How to discover the correct weights?</h3>
<ul>
<li>Gradient Descent:
<ul>
<li>Finding the <em>minimum of a function</em>;
<ul>
<li>Look for the best weights values, <em>minimizing the error</em>;</li>
</ul>
</li>
<li>Takes steps <em>proportional to the negative of the gradient</em> of the function at the current point.</li>
<li>Gradient is a vector that is <em>tangent of a function</em> and points in the direction of greatest increase of this function.</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-14">Artificial Neural Networks</h2>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>In mathematics, gradient is defined as <em>partial derivative for every input variable</em> of function;</li>
<li><em>Negative gradient</em> is a vector pointing at the <em>greatest decrease</em> of a function;</li>
<li><em>Minimize a function</em> by iteratively moving a little bit in the direction of negative gradient;</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-15">Artificial Neural Networks</h2>
<h3 id="gradient-descent-1">Gradient Descent</h3>
<ul>
<li>With a single weight:</li>
</ul>
<center><img src="gd1.jpeg" width="500px"/></center>
</section><section>
<h2 id="finding-values-of-w_0-and-w_1-3">Finding values of $w_0$ and $w_1$</h2>
<h3 id="gradient-descent-technique">Gradient Descent Technique</h3>
<ul>
<li>Iterative calculation of the matrix $W$ with:</li>
</ul>
<p>$w_0^{(t+1)} = w_0^{(t)} - \alpha \frac{\partial RSS}{\partial w_0}$</p>
<p>$w_1^{(t+1)} = w_1^{(t)} - \alpha \frac{\partial RSS}{\partial w_1}$</p>
<ul>
<li>Where $\alpha$ is the Learning Rate, i.e., the step size towards the minimum cost value.</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-16">Artificial Neural Networks</h2>
<h3 id="gradient-descent-2">Gradient Descent</h3>
<iframe src="manual_NN2.html" height="500px" width="800px">
</iframe>
</section><section>
<h2 id="artificial-neural-networks-17">Artificial Neural Networks</h2>
<h3 id="perceptron">Perceptron</h3>
<ul>
<li>In 1958, Frank Rosenblatt proposed an algorithm for training the perceptron.</li>
<li>Simplest form of Neural Network;</li>
<li>One unique neuron;</li>
<li>Adjustable Synaptic weights</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-18">Artificial Neural Networks</h2>
<h3 id="perceptron-1">Perceptron</h3>
<ul>
<li>Classification of observations into two classes:</li>
</ul>
<center><img src="perceptron1.png" height="350px"/></center>
<h6 id="images-taken-from-a-hrefhttpstowardsdatasciencecomperceptron-learning-algorithm-d5db0deab975-target_blanktowards-data-sciencea">Images Taken from <a href="https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975" target="_blank">Towards Data Science</a></h6>
</section><section>
<h2 id="artificial-neural-networks-19">Artificial Neural Networks</h2>
<h3 id="perceptron-2">Perceptron</h3>
<ul>
<li>Classification of observations into two classes:</li>
</ul>
<center><img src="perceptron2.png" height="350px"/></center>
<h6 id="images-taken-from-a-hrefhttpstowardsdatasciencecomperceptron-learning-algorithm-d5db0deab975-target_blanktowards-data-sciencea-1">Images Taken from <a href="https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975" target="_blank">Towards Data Science</a></h6>
</section><section>
<h2 id="artificial-neural-networks-20">Artificial Neural Networks</h2>
<h3 id="perceptron-3">Perceptron</h3>
<ul>
<li>E.g, the OR function:</li>
</ul>
<center><img src="or1.png" width="550px"/></center>
<h4 id="find-the-w_i-values-that-could-solve-the-or-problem">Find the $w_i$ values that could solve the or problem.</h4>
</section><section>
<h2 id="artificial-neural-networks-21">Artificial Neural Networks</h2>
<h3 id="perceptron-4">Perceptron</h3>
<ul>
<li>E.g, the OR function:</li>
</ul>
<br />
<center><img src="or2.png" width="550px"/></center>
</section><section>
<h2 id="artificial-neural-networks-22">Artificial Neural Networks</h2>
<h3 id="perceptron-5">Perceptron</h3>
<ul>
<li>One possible solution $w_0=-1$, $w_1=1.1$, $w_2=1.1$:</li>
</ul>
<center><img src="or4.png" width="450px"/></center>
</section><section>
<h2 id="artificial-neural-networks-23">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<ul>
<li><em>High-level</em> neural networks API;</li>
<li>Capable of running on top of <em>TensorFlow</em>, <em>CNTK</em>, or <em>Theano</em>;</li>
<li>Focus on enabling <em>fast experimentation</em>;
<ul>
<li>Go from idea to result with the <em>least possible delay</em>;</li>
</ul>
</li>
<li>Runs seamlessly on <em>CPU</em> and <em>GPU</em>;</li>
<li>Compatible with: <em>Python 2.7-3.12</em> and <em>R</em>;</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-24">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka-1">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<ul>
<li>Use the implementation of the tensorflow:
<ul>
<li>Create a sequential model (perceptron)</li>
</ul>
</li>
</ul>
<pre><code class="language-python"># Import the Sequential model
from tensorflow.keras.models import Sequential

# Instantiate the model
model = Sequential()
</code></pre>
</section><section>
<h2 id="artificial-neural-networks-25">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka-2">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<ul>
<li>Create a single layer with a single neuron:
<ul>
<li><code>units</code> represent the number of neurons;</li>
</ul>
</li>
</ul>
<pre><code class="language-python"># Import the Dense layer
from tensorflow.keras.layers import Dense

# Add a forward layer to the model
model.add(Dense(units=1, input_dim=2))
</code></pre>



<aside class="notes"><ul>
<li>Dense means a fully connected layer.</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-26">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka-3">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<ul>
<li>Compile and train the model
<ul>
<li>The compilation creates a <a
href="https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9" target="_blank">computational graph</a> of the training;</li>
</ul>
</li>
</ul>
<pre><code class="language-python"># Specify the loss function (error) and the optimizer
#   (a variation of the gradient descent method)
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;sgd&quot;)

# Fit the model using the train data and also
#   provide the expected result
model.fit(x=train_data_X, y=train_data_Y)
</code></pre>



<aside class="notes"><ul>
<li>Computational Graphs:
<ul>
<li>Nodes represent both inputs and operations;</li>
<li>Even relatively “simple” deep neural networks have hundreds of thousands of nodes and edges;</li>
<li>Lots of operations can run in parallel;
<ul>
<li>Example: $(x<em>y)+(w</em>z)$</li>
</ul>
</li>
<li>Makes it easier to create an auto diferentiation strategy;</li>
<li>We can user <code>verbose=1</code> to increase the output;</li>
</ul>
</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-27">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka-4">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<ul>
<li>Evaluate the quality of the model:</li>
</ul>
<pre><code class="language-python"># Use evaluate function to get the loss and other metrics that the framework
#  makes available
loss_and_metrics = model.evaluate(train_data_X, train_data_Y)
print(loss_and_metrics)
#0.4043288230895996

# Do a prediction using the trained model
prediction = model.predict(train_data_X)
print(prediction)
# [[-0.25007164]
#  [ 0.24998784]
#  [ 0.24999022]
#  [ 0.7500497 ]]
</code></pre>



<aside class="notes"><p>We can use verbose during the evaluate</p>
</aside>
</section><section>
<h2 id="artificial-neural-networks-28">Artificial Neural Networks</h2>
<h3 id="activation-functions">Activation Functions</h3>
<ul>
<li>Describes <em>whether or not the neuron fires</em>, i.e., if it forwards its value for the next neuron layer;</li>
<li>Historically they translated the output of the neuron into either 1 (On/active) or 0 (Off) - Step Function:</li>
</ul>
<pre><code class="language-r">if(prediction[i]&gt;0.5){
  return 1
}
return 0
</code></pre>
</section><section>
<h2 id="artificial-neural-networks-29">Artificial Neural Networks</h2>
<h3 id="the-a-hrefhttpskerasio-target_blankkeras-frameworka-5">The <a href="https://keras.io" target="_blank">Keras framework</a></h3>
<h4 id="exercise">Exercise:</h4>
<p>Run the example of the Jupyter notebook:
<br />
<a href="https://colab.research.google.com/drive/1hNOR60jfru-b0Vb-ec-Y_yF9pyuy8Wtj?usp=sharing" target="_blank">Perceptron - OR</a></p>
</section><section>
<h2 id="artificial-neural-networks-30">Artificial Neural Networks</h2>
<h3 id="perceptron-6">Perceptron</h3>
<h4 id="exercise-1">Exercise:</h4>
<ul>
<li>What about the <em>AND</em> function?</li>
</ul>
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>$y$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>


<span class='fragment ' ><a href="https://colab.research.google.com/drive/1aOUtlD401TH6AfwBUs9XeCrxFy5PVLBa?usp=sharing" target="_blank">My solution</a>.</span>


</section><section>
<h2 id="artificial-neural-networks-31">Artificial Neural Networks</h2>
<h3 id="perceptron---what-it-emcant-doem">Perceptron - What it <em>can&rsquo;t do</em>!</h3>
<ul>
<li>The <em>XOR</em> function:</li>
</ul>
<center><img src="xor1.png" width="500px"/></center>
<p>Check-out what happens when we try to use the same architecture for solving the
XOR function <a
href="https://colab.research.google.com/drive/1NKIpV-SZ38SU6szy_e2hZNBG7MC2_d9G?usp=sharing"
target="_blank">here</a>.</p>
</section><section>
<h2 id="artificial-neural-networks-32">Artificial Neural Networks</h2>
<h3 id="understanding-the-training">Understanding the training</h3>
<ul>
<li>Plotting the training progress of the XOR ANN:</li>
</ul>
<pre><code class="language-python">history = model.fit(x=X_data, y=Y_data, epochs=2500, verbose=0)
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.title('Model Training Progression')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Loss'], loc='upper left')
plt.show()
</code></pre>
<center><a href="loss_trainning2.png" target="_blank"><img src="loss_trainning2.png" width="250px" /></a></center>



<aside class="notes"><ul>
<li>This is called the <em>learning curve</em>;</li>
<li>In the case of the XOR. <em>What is wrong with that?</em></li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-33">Artificial Neural Networks</h2>
<h3 id="activation-functions-1">Activation Functions</h3>
<ul>
<li><em>Multiply the input</em> by its <em>weights</em>, <em>add the bias</em> and <em>applies activation</em>;</li>
<li>Sigmoid, Hyperbolic Tangent, Rectified Linear Unit;</li>
<li><em>Differentiable function</em> instead of the step function;</li>
</ul>
<center> <img src="activation_functions.png" width="500px"/></center>



<aside class="notes"><ul>
<li>
<p>With this modification, a multi-layered network of perceptrons would become
differentiable. Hence gradient descent could be applied to minimize the
network’s error and the chain rule could “back-propagate” proper error
derivatives to update the weights from every layer of the network.</p>
</li>
<li>
<p>At the moment, one of the most efficient ways to train a multi-layer neural
network is by using gradient descent with backpropagation. A requirement for
backpropagation algorithm is a differentiable activation function. However, the
Heaviside step function is non-differentiable at x = 0 and it has 0 derivative
elsewhere. This means that gradient descent won’t be able to make a progress in
updating the weights.</p>
</li>
<li>
<p>The main objective of the neural network is to learn the values of the weights
and biases so that the model could produce a prediction as close as possible to
the real value. In order to do this, as in many optimisation problems, we’d
like a small change in the weight or bias to cause only a small corresponding
change in the output from the network. By doing this, we can continuously
tweaked the values of weights and bias towards resulting the best
approximation. Having a function that can only generate either 0 or 1 (or yes
and no), won&rsquo;t help us to achieve this objective.</p>
</li>
</ul>
</aside>
</section><section>
<h2 id="some-examples-of-activation-functions">Some examples of activation functions:</h2>
<p><strong>Logistic function (sigmoid):</strong></p>
<p>$g(a) = \frac{1}{1+e^{-a}}, \text{ such that } g(a)\in[0,1]$</p>
<p><br /><br /></p>
<p><strong>Hyperbolic tangent function:</strong></p>
<p>$g(a) = 2\sigma(2a)-1, \text{ such that } g(a)\in[-1,1]$
where $\sigma(x)$ corresponds to the <strong>logistic function</strong>.</p>
</section><section>
<h2 id="activation-functions-2">Activation Functions</h2>
<p>It is desirable that an activation function is <strong>differentiable</strong>.</p>
<p>How to choose it? 

<span class='fragment ' >It depends on your application.</span>

</p>
<img src="Neural_Networks_Fig3.png" width=250px/>
<br/>
<font size="5">  
<ul>
<li>Assume $g(a) = \frac{1}{1+e^{-a}}$.</li>
<li>We have that $g^\prime(a) = g(a)(1-g(a))$.</li>
<li>Notice that $g^\prime(a)$ <strong>saturates</strong> when $a &gt; 5$ or $a &lt; -5$.</li>
<li>Furthermore, $g^\prime(a) &lt; 1$ for all $a$. This means that for networks with many layers, the gradient tends to <strong>vanish</strong> during training.</li>
</ul>
</font>
</section><section>
<h2 id="activation-functions-3">Activation Functions</h2>
<img src="Neural_Networks_Fig4.png" width=250px/>
<ul>
<li>Assume $g(a) = 2\sigma(2a)-1$.</li>
<li>We have that $g^\prime(a) = 1-g^2(a)$.</li>
<li>Although saturation occurs, $g^\prime(a)$ reaches higher values, even reaching a maximum of 1 when $a = 0$.</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-34">Artificial Neural Networks</h2>
<h3 id="the-bias-1">The Bias</h3>
<center><img src="bias1.png" width="600px"/></center>
</section><section>
<h2 id="artificial-neural-networks-35">Artificial Neural Networks</h2>
<h3 id="the-bias-2">The Bias</h3>
<p>Give even more power to our model</p>
<center><img src="bias2.png" width="450px"/></center>
</section><section>
<h2 id="artificial-neural-networks-36">Artificial Neural Networks</h2>
<h3 id="perceptron---solving-the-xor-problem">Perceptron - Solving the XOR problem</h3>
<ul>
<li>3D example of the solution of learning the OR function:
<ul>
<li>Using <em>Sigmoid</em> function;</li>
</ul>
</li>
</ul>
<center> <img src="or5.png" width="600px"/></center>



<aside class="notes"><p>That creates a <strong>hyperplane</strong> that separates the classes;</p>
</aside>
</section><section>
<h2 id="artificial-neural-networks-37">Artificial Neural Networks</h2>
<h3 id="perceptron---solving-the-xor-problem-1">Perceptron - Solving the XOR problem</h3>
<ul>
<li>Maybe there is a combination of functions that could create hyperplanes that separate the <em>XOR</em> classes:
<ul>
<li>By increasing the number of layers we increase the complexity of the function represented by the ANN:</li>
</ul>
</li>
</ul>
<center><a href="xor2.png" target="_blank"><img src="xor2.png" width="580px"/></a></center>



<aside class="notes"><p>Now, there are 2 hyperplanes, that when put together, can perfectly separate the classes;</p>
</aside>
</section><section>
<h2 id="artificial-neural-networks-38">Artificial Neural Networks</h2>
<h3 id="perceptron---solving-the-xor-problem-2">Perceptron - Solving the XOR problem</h3>
<ul>
<li>The combination of the layers:</li>
</ul>
<center><a href="xor3.png" target="_blank"><img src="xor3.png" width="300px"/></a></center>



<aside class="notes"><ul>
<li>That is what people mean when they say we don&rsquo;t know how deep neural networks
work. We know that it is a composition of functions, but the shape of that
remains a little bit hard to define;</li>
<li>Yesterday we saw polynomial transformation of features - in that we saw that
we changed the shape of the regression line being built;</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-39">Artificial Neural Networks</h2>
<h4 id="emmultilayer-perceptronsem---increasing-the-model-power"><em>Multilayer Perceptrons</em> - Increasing the model power</h4>
<ul>
<li>
<p>Typically represented by composing many different
functions:
$$y = f^{(3)}(f^{(2)}(f^{(1)}(x)))$$</p>
</li>
<li>
<p>The <em>depth</em> of the network - the <em>deep</em> in deep learning! (-;</p>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-40">Artificial Neural Networks</h2>
<h4 id="emmultilayer-perceptronsem---increasing-the-model-power-1"><em>Multilayer Perceptrons</em> - Increasing the model power</h4>
<ul>
<li>Information flows from $x$ , through computations and finally to $y$.</li>
<li>No feedback!</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-41">Artificial Neural Networks</h2>
<h3 id="perceptron---solving-the-xor-problem-3">Perceptron - Solving the XOR problem</h3>
<ul>
<li>Implementing an ANN that can solve the XOR problem:
<ul>
<li>Add a new layer with a larger number of neurons:</li>
</ul>
</li>
</ul>
<pre><code class="language-python">...
#Create a layer with 4 neurons as output
model.add(Dense(units=4), activation=&quot;sigmoid&quot;, input_dim=2)

# Connect to the first layer that we defined
model.add(Dense(units=1, activation=&quot;sigmoid&quot;)
</code></pre>
<p>Let&rsquo;s check if that solves our XOR problem <a
href="https://colab.research.google.com/drive/1hpRRtJuC78uPXJE68oOjRaM03LVV_rgo?usp=sharing"
target="_blank">here</a>.</p>



<aside class="notes"><p>Train for little steps and then increase the number of epochs</p>
</aside>
</section><section>
<h2 id="artificial-neural-networks-42">Artificial Neural Networks</h2>
<h3 id="understanding-the-training-1">Understanding the training</h3>
<ul>
<li>Plot the architecture of the network:</li>
</ul>
<pre><code class="language-python">tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)
</code></pre>
<center><img src="nn_architecture.png" width="300px" /></center>



<aside class="notes"><p>The ? means that they take as much examples as possible;</p>
</aside>
</section><section>
<h2 id="artificial-neural-networks-43">Artificial Neural Networks</h2>
<h3 id="problems-with-the-training-procedure">Problems with the training procedure:</h3>
<ul>
<li>Saddle points:
<ul>
<li>No matter how long you train your model for, <em> the error remains (almost) constant!</em></li>
</ul>
</li>
</ul>
<center><a href="saddle.png" target="_blank"><img src="saddle.png" width="300px" /></a></center>



<aside class="notes"><ul>
<li>That eventually happens because of a bad optimization function;</li>
<li>Imagine that you could add momentum to the gradient descent - probably it
could continue updating;</li>
<li>In the XOR case, there are 16 local minimums that have the highest conversion
if the weights are initialized between 0.5 and 1.</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-44">Artificial Neural Networks</h2>
<h3 id="optimization-alternatives">Optimization alternatives</h3>
<ul>
<li>The Gradient Descent is <em>not always the best option</em> to go with:
<ul>
<li>Only does the update after <em>calculating the derivative for the whole
dataset</em>;</li>
<li>Can take a <em>long time to find the minimum</em> point;</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-45">Artificial Neural Networks</h2>
<h3 id="optimization-alternatives-1">Optimization alternatives</h3>
<ul>
<li>The <a href="gd.gif" target="_blank">Gradient Descent</a> is <em>not always the best option</em> to go with:
<ul>
<li>For non-convex surfaces, it may only find the local minimums - <a href="gd2.gif" target="_bank">the saddle situation</a>;</li>
<li><strong><em>Vectorization</em></strong></li>
</ul>
</li>
</ul>
<p><a href="vectorization.jpeg" target="_blank"><center><img src="vectorization.jpeg" width="450px" /></center></a></p>



<aside class="notes"><ul>
<li>For large datasets, the vectorization of data doesn’t fit into memory.</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-46">Artificial Neural Networks</h2>
<h3 id="optimization-alternatives-2">Optimization alternatives</h3>
<ul>
<li>Gradient Descent alternatives:
<ul>
<li><a href="sgd.gif" target="_blank">Stochastic Gradient Descent</a>: updates at each input;</li>
<li><a href="minibatch.gif" target="_blank">Minibatch Gradient Descent</a>: updates after reading a batch of examples;
<br /><br /></li>
</ul>
</li>
</ul>
<center>
<h6 id="animations-taken-from-vikashraj-luhaniwal-a-hrefhttpstowardsdatasciencecomwhy-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096-target--_blankposta">Animations taken from Vikashraj Luhaniwal <a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096" target = "_blank">post</a>.</h6>
</center>



<aside class="notes"><p>Minibatch:</p>
<ul>
<li>Updates are less noisy compared to SGD which leads to better convergence.</li>
<li>A high number of updates in a single epoch compared to GD so less number of epochs are required for large datasets.</li>
<li>Fits very well to the processor memory which makes computing faster.</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-47">Artificial Neural Networks</h2>
<h3 id="optimization-alternatives-3">Optimization alternatives</h3>
<h6 id="adaptative-learning-rates">Adaptative Learning Rates:</h6>
<ul>
<li><a href="adagrad.gif" target="_blank">Adagrad</a>, <a href="rmsprop.gif" target="_blank">RMSProp</a>, <a href="adam.gif" target="_blank">Adam</a>;</li>
</ul>
<h6 id="heading-1"></h6>
<p><br /><br /></p>
<center>
<h6 id="animations-taken-from-vikashraj-luhaniwal-a-hrefhttpstowardsdatasciencecomwhy-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096-target--_blankposta-1">Animations taken from Vikashraj Luhaniwal <a href="https://towardsdatascience.com/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096" target = "_blank">post</a>.</h6>
</center>



<aside class="notes"><ul>
<li>For Adagrad:
<ul>
<li>Parameters with small updates(sparse features) have high learning rate whereas the parameters with large updates(dense features) have low learning rateupdates at each input;</li>
<li>The learning rate decays very aggressively</li>
</ul>
</li>
<li>RMSProp: A large number of oscillations with high learning rate or large gradient</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-48">Artificial Neural Networks</h2>
<h3 id="multilayer-perceptron---xor">Multilayer Perceptron - XOR</h3>
<ul>
<li>Try another optimizer:</li>
</ul>
<pre><code class="language-python">model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;)
</code></pre>
<p>My <a href="https://colab.research.google.com/drive/1tsK3aESxc0xGTVLhry-vwMpK2j8mRl-o?usp=sharing" target="_blank">solution</a></p>
</section><section>
<h2 id="artificial-neural-networks-49">Artificial Neural Networks</h2>
<h3 id="predicting-probabilities">Predicting probabilities</h3>
<ul>
<li>Imagine that we have <em>more than 2 classes</em> to output;</li>
<li>One of the <em>most popular usages</em> for ANN;</li>
</ul>
<center><a href="classification_example.jpeg" target="_blank"><img src="classification_example.jpeg" width="300px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-50">Artificial Neural Networks</h2>
<h3 id="predicting-probabilities-1">Predicting probabilities</h3>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank">Softmax</a> function;</li>
<li>Takes an array and outputs a probability distribution, i.e., <em>the probability
of the input example belonging to each of the classes</em> in my problem;</li>
<li>One of the activation functions available at <code>Keras</code>:</li>
</ul>
<pre><code class="language-r">layer_dense(units = 2, activation = 'softmax')
</code></pre>



<aside class="notes"><ul>
<li>Softmax - function that takes as input a vector of K real numbers, and normalizes it into a probability distribution</li>
</ul>
</aside>
</section><section>
<h2 id="the-softmax-activation">The Softmax Activation</h2>
<ul>
<li>The softmax function $\sigma:\mathbb{R}^K\rightarrow [0,1]^K$ is a generalization of the logistic function, where $K$ corresponds to the number of classes</li>
</ul>
<img width="250" src="CNN_Fig14.png" />
<ul>
<li><strong>Why softmax and not the logistic function?</strong>
<ul>
<li>Usually, the logistic function is applied to each output neuron without considering all the others. In this case, softmax results in a probability of the neuron of each class responding to an input stimulus</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-51">Artificial Neural Networks</h2>
<h3 id="loss-functions">Loss functions</h3>
<ul>
<li>For regression problems
<ul>
<li>Mean squared error is <em>not always the best one to go</em>;
<ul>
<li>What if we have a three classes problem?</li>
</ul>
</li>
<li>Alternatives: <code>mean_absolute_error</code>, <code>mean_squared_logarithmic_error</code></li>
</ul>
</li>
</ul>



<aside class="notes"><ul>
<li>logarithm means changing scale as the error can grow really fast;</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-52">Artificial Neural Networks</h2>
<h3 id="loss-functions-1">Loss functions</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank">Cross Entropy</a> loss:
<ul>
<li>Default loss function to use for binary classification problems.</li>
<li>Measures the <em>performance of a model</em> whose output is a probability value between 0 and 1;</li>
<li><em>Loss increases</em> as the <em>predicted probability diverges</em> from the actual label;</li>
<li>A <em>perfect model</em> would have a log loss of 0;</li>
</ul>
</li>
</ul>



<aside class="notes"><ul>
<li>
<p>As the correct predicted probability decreases, however, the log loss increases rapidly:</p>
</li>
<li>
<p>In case the model has to answer 1, but it does with a very low probability;</p>
</li>
<li>
<p>If you have events and probabilities, how likely is it that the events happen based on the probabilities?</p>
<ul>
<li>If it is very likely, we have a small cross-entropy and if it is not likely we have a high cross-entropy.</li>
</ul>
</li>
</ul>
</aside>
</section><section>
<h2 id="artificial-neural-networks-53">Artificial Neural Networks</h2>
<h3 id="what-about-the-overfitting">What about the overfitting?</h3>
<img src="overfitting.png" width="300px"/>
</section><section>
<h2 id="interpretation-of-the-test-set">Interpretation of the Test Set</h2>
<h3 id="does-perfect-metric-on-the-test-set-mean-the-model-is-perfect">Does Perfect Metric on the Test Set Mean the Model is Perfect?</h3>
<p><strong>Not necessarily:</strong> In no non-trivial problem will you have access to a completely representative database of the problem.
Evaluating with a test set <em>alleviates</em> but doesn&rsquo;t solve the problem.
There will never be enough examples to perfectly model the phenomenon.</p>
</section><section>
<h2 id="analysis-of-model-error">Analysis of Model Error</h2>
<h3 id="types-of-errors">Types of Errors</h3>
<ul>
<li>Prediction error can be divided into three parts:
<ul>
<li><strong>Irreducible Error:</strong> cannot be eliminated regardless of the algorithm used.
<ul>
<li><strong>Introduced from the chosen problem framework</strong>.</li>
<li><strong>Caused by unknown factors</strong>.</li>
</ul>
</li>
<li><strong>Bias Error:</strong> assumptions made by a model to make the target function easier to learn.</li>
<li><strong>Variance Error:</strong> the amount the target function estimate will change if different training data are used.</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="analysis-of-model-error-1">Analysis of Model Error</h2>
<h3 id="bias-error">Bias Error</h3>
<ul>
<li>Difference between the expected (or average) prediction of our model and the correct value we are trying to predict.</li>
<li>Imagine repeating the entire model-building process more than once:
<ul>
<li><strong>Each time you gather new data and perform a new analysis, you create a new model.</strong></li>
<li><strong>Due to randomness in the underlying data sets, resulting models will have a variety of predictions.</strong></li>
<li><strong>Measures how far, on average, predictions of these models are from the correct value.</strong></li>
</ul>
</li>
<li>Our model has bias if it systematically predicts below or above the target variable.</li>
</ul>
</section><section>
<h2 id="analysis-of-model-error-2">Analysis of Model Error</h2>
<h3 id="variance-error">Variance Error</h3>
<ul>
<li>In a sense, captures the <strong>model&rsquo;s generalization capability</strong>.</li>
<li>How much our prediction would change if we trained it with different data.</li>
<li>Ideally, it shouldn&rsquo;t change much from one training data set to the next.</li>
<li>Algorithms with high variance are <strong>strongly influenced by the specifics of training data.</strong></li>
<li>Generally, nonlinear machine learning algorithms that are very flexible have <strong>high variance.</strong>
<ul>
<li><strong>For example, Polynomial Regression with high-degree polynomials!</strong></li>
</ul>
</li>
</ul>
</section><section>
<h2 id="the-bias-variance-tradeoff">The Bias-Variance Tradeoff</h2>
<ul>
<li>
<p><strong>Bias</strong> error arises due to incorrect assumptions made by the learning algorithm. Excessive bias can lead the algorithm to overlook important connections between features and target outcomes, resulting in underfitting.</p>
</li>
<li>
<p><strong>Variance</strong> represents the error stemming from the algorithm&rsquo;s susceptibility to minor variations in the training dataset. Elevated variance might occur when the algorithm models the random noise present in the training data, causing overfitting.</p>
</li>
</ul>
</section><section>
<h2 id="dilemma-variance-vs-bias">Dilemma: Variance vs Bias</h2>
<ul>
<li>Low bias: suggests fewer assumptions about the shape of the target function.
<ul>
<li><strong>Regression Trees, KNN Regression</strong></li>
</ul>
</li>
<li>High bias: suggests more assumptions about the shape of the target function.
<ul>
<li><strong>Linear Regression, Logistic Regression</strong></li>
</ul>
</li>
<li>Low variance: suggests small changes in the estimate of the target function with changes in the training data set.
<ul>
<li><strong>Linear Regression, Logistic Regression</strong></li>
</ul>
</li>
<li>High variance: suggests large changes in the estimate of the target function with changes in the training data set.
<ul>
<li><strong>Regression Trees, KNN Regression</strong></li>
</ul>
</li>
</ul>
</section><section>
<h2 id="dilemma-variance-vs-bias-1">Dilemma: Variance vs Bias</h2>
<img src="variance_x_bias.png" width="400px"/>
<br />
<ul>
<li>Increasing bias will decrease variance.</li>
<li>Increasing variance will decrease bias.</li>
</ul>
</section><section>
<h2 id="dilemma-variance-vs-bias-2">Dilemma: Variance vs Bias</h2>
<h2 id="tradeoff">Tradeoff</h2>
<img src="bias_variance1.png" width="450px"/>
</section><section>
<h2 id="dilemma-variance-vs-bias-3">Dilemma: Variance vs Bias</h2>
<ul>
<li>A very simple model with few parameters has high Bias and low Variance.</li>
<li>A complex model with a large number of parameters will have high Variance and low Bias.</li>
<li>One should aim for balance, avoiding overfitting while not underfitting the data.</li>
</ul>
</section><section>
<h2 id="dilemma-variance-vs-bias-4">Dilemma: Variance vs Bias</h2>
<ul>
<li>Models should try to <strong>generalize</strong> beyond what is observed in the training set.</li>
<li><strong>Regularization</strong> plays a role in controlling the overfitting of classifiers.</li>
</ul>
</section><section>
<h2 id="visualizing-the-overfitting">Visualizing the Overfitting</h2>
<h3 id="how-decision-trees-change-after-removing-a-few-examples">How Decision Trees change after removing a few examples</h3>
<img src="fit_tree.png" width="300px"/>
<img src="fit_tree_mod.png" width="300px"/>
</section><section>
<h2 id="regularization">Regularization</h2>
<ul>
<li>Decreases variance by reducing learning effectiveness.</li>
<li>Penalizes model complexity.</li>
<li>Nearly all learning algorithms have some form of regularization mechanism.</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-54">Artificial Neural Networks</h2>
<h3 id="dealing-with-overfitting">Dealing with overfitting</h3>
<ul>
<li><em>Dropout</em> layers:
<ul>
<li>Randomly <em>disable</em> some of the neurons during the training passes;</li>
</ul>
</li>
</ul>
<center><a href="dropout.gif" target="_blank"><img src="dropout.gif" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-55">Artificial Neural Networks</h2>
<h3 id="dealing-with-overfitting-1">Dealing with overfitting</h3>
<ul>
<li><em>Dropout</em> layers:</li>
</ul>
<pre><code class="language-python"># Drop half of the neurons outputs from the previous layer
model.add(Dropout(0.5))
</code></pre>



<aside class="notes"><ul>
<li>“drops out” a random set of activations in that layer by setting them to zero;</li>
<li>forces the network to be redundant;</li>
<li>the net should be able to provide the right classification for a specific example even if some of the activations are dropped out;</li>
</ul>
</aside>
</section><section>
<h2 id="how-does-it-realy-do-it">How does it realy do it?</h2>
<video width="620" height="440" controls>
  <source src="ann.mp4" type="video/mp4">
</video>
</section><section>
<h2 id="artificial-neural-networks-56">Artificial Neural Networks</h2>
<h3 id="larger-example">Larger Example</h3>
<ul>
<li>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset: database of handwritten digits;</li>
<li>Dataset included in Keras;</li>
</ul>
<center><a href="mnist.png" target="_blank"><img src="mnist.png" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-57">Artificial Neural Networks</h2>
<h3 id="the-mnist-mlp">The MNIST MLP</h3>
<ul>
<li>Try to improve the classification results using <a href="https://colab.research.google.com/drive/1AnGJz_R0PJF0d83ye_3y7NPGuX5YipBi?usp=sharing" target="_blank">this notebook</a>:</li>
<li>Things to try:
<ul>
<li>Increase the number of neurons at the first layer;</li>
<li>Change the optimizer and the loss function;</li>
<li>Try <code>categorical_crossentropy</code> and <code>rmsprop</code> optimizer;</li>
<li>Try adding some extra layers;</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="artificial-neural-networks-58">Artificial Neural Networks</h2>
<h3 id="the-mnist-mlp-1">The MNIST MLP</h3>
<ul>
<li>
<p>Try to improve the classification results using <a href="https://colab.research.google.com/drive/1AnGJz_R0PJF0d83ye_3y7NPGuX5YipBi?usp=sharing" target="_blank">this notebook</a>:</p>
</li>
<li>
<p>Things to try:</p>
<ul>
<li>Try addind <code>Dropout</code> layers;</li>
<li>Increase the number of <code>epochs</code>;</li>
<li>Try to <em>normalize the data</em>!</li>
</ul>
</li>
<li>
<p>What is the best accuracy?</p>
</li>
</ul>


<span class='fragment ' ><ul>
<li><a href="https://colab.research.google.com/drive/1LnkhSA7XbEWMNdaebOXxsOENr6m-0vpZ?usp=sharing" target="_blank">My solution</a>.</li>
</ul>
</span>


</section><section>
<h1 id="mini-projects">Mini Projects</h1>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="cms.png">
  
<h1 id="span-stylecolorfff-particle-physicsspan"><span style="color:#fff;"> Particle Physics</span></h1>
</section><section>
<h2 id="artificial-neural-networks-59">Artificial Neural Networks</h2>
<h3 id="the-particle-physics-project">The Particle Physics Project</h3>
<center><a href="atlas_particle_shower.jpg" target="_blank"><img src="atlas_particle_shower.jpg" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-60">Artificial Neural Networks</h2>
<h3 id="the-particle-physics-project-1">The Particle Physics Project</h3>
<center><a href="jet-images.png" target="_blank"><img src="jet-images.png" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-61">Artificial Neural Networks</h2>
<h3 id="the-particle-physics-project-2">The Particle Physics Project</h3>
<ul>
<li>Quantum Chromodynamics</li>
</ul>
<center><a href="qcd.png" target="_blank"><img src="qcd.png" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-62">Artificial Neural Networks</h2>
<h3 id="signal-vs-background">Signal VS Background</h3>
<center><a href="backgroundVSsignal.png" target="_blank"><img src="backgroundVSsignal.png" width="700px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-63">Artificial Neural Networks</h2>
<h3 id="signal-vs-background-1">Signal VS Background</h3>
<p>Run this <a href="https://colab.research.google.com/drive/1cbiRS7ax1k1mzqk6NioSuQgOiLWJIVx3?usp=sharing" target="_blank">Jupyter Notebook</a> for performing the Jet Classification.</p>
</section>

<section data-noprocess data-shortcode-slide
      data-background-image="COVID19_CT.jpg">
  
<h1 id="span-stylecolorff0000covid19-chest-ct-image-processingspan"><span style="color:#ff0000;">COVID19 Chest CT Image Processing</span></h1>
</section><section>
<h2 id="artificial-neural-networks-64">Artificial Neural Networks</h2>
<h3 id="covid19-diagnosis">COVID19 Diagnosis</h3>
<center><a href="COVIDCT1.png" target="_blank"><img src="COVIDCT1.png" width="500px"/></a></center>
</section><section>
<h2 id="artificial-neural-networks-65">Artificial Neural Networks</h2>
<h3 id="covid19-diagnosis-1">COVID19 Diagnosis</h3>
<p>Run this <a href="https://colab.research.google.com/drive/1rXZP2XofMS3ijiOkrsJSn4dUK1zzz68t?usp=sharing" target="_blank">Jupyter Notebook</a> for performing the CT Image Classification.</p>



<aside class="notes"><p>Remember to check the impact of normallization
<code>x_train = x_train / 255.0   # Pixel normalization</code></p>
</aside>
</section><section>
<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>
</section><section>
<h2 id="introduction">Introduction</h2>
<ul>
<li><strong>Deep learning</strong>
<ul>
<li>Branch of machine learning that uses neural networks with <em>multiple layers</em></li>
<li>Employ layers to progressively learn <em>different levels of features</em> from input data</li>
<li>Abstraction increase as more layers are used for <em>feature extraction</em> and learning</li>
</ul>
</li>
</ul>


<span class='fragment ' ><ul>
<li>Among deep learning techniques, special attention has been given to <em>Convolutional Neural Networks</em> (CNNs).</li>
<li>These models have a <em>high capacity for data representation</em>, yielding promising results in numerous fields of knowledge.</li>
</ul>
</span>


</section><section>
<h2 id="convolutional-neural-networks-1">Convolutional Neural Networks</h2>
<ul>
<li>The basic idea is to use <em>raw data</em> as input</li>
<li>Allow the network to learn the <em>most important features</em> for the problem at hand.</li>
<li><em>Eliminates the need to manually extract features</em> (handcrafted features).</li>
</ul>
<img width="450" src="CNN_Fig1.png" />
</section><section>
<h2 id="convolutional-neural-networks-2">Convolutional Neural Networks</h2>
<ul>
<li>Generally composed of two  modules:


<span class='fragment ' ><ol>
<li>Feature learning: performed through convolution, pooling, and activation operations.</li>
</ol>
</span>




<span class='fragment ' ><ol start="2">
<li>Classification: typically  fully connected layers and a softmax output layer.</li>
</ol>
</span>

</li>
</ul>


<span class='fragment ' ><img width="500" src="CNN_Fig2.png" />
</span>


</section><section>
<h2 id="convolutional-neural-networks-3">Convolutional Neural Networks</h2>
<ul>
<li>What makes CNNs so interesting for various pattern classification tasks?</li>
<li><strong>feature learning</strong>
<ul>
<li>Important information (like texture) is learned at different levels</li>
</ul>
</li>
<li>Less susceptible to <em>rotation</em>, <em>translation</em>, and <em>scale issues</em>.</li>
<li>To understand how they work, we will first study the feature learning layers
<ul>
<li>(i) convolution, (ii) pooling, and (iii) activation operations.</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="feature-learning">Feature Learning</h2>
<h3 id="convolution">Convolution</h3>
<ul>
<li><em>Convolution</em> are widely used in image processing and computer vision tasks, such as image filtering (blurring and noise) and edge detection
<br /><br /></li>
</ul>
<center>
<img width="650" src="CNN_Fig3.png" />
</center>
</section><section>
<h2 id="convolution-1">Convolution</h2>
<img width="450" src="CNN_Fig4.png" />
<ul>
<li>The central position of the 9x9 input is replaced by its convolution with the 5x5 mask</li>
<li>The value is stored in the 5x5 output matrix (feature map).</li>
<li>The procedure is repeated until the entire input matrix has been evaluated.</li>
</ul>
</section><section>
<h2 id="what-are-the-hyperparameters-involved-in-a-convolution-operation">What are the hyperparameters involved in a convolution operation?</h2>


<span class='fragment ' ><h3 id="parameters-vs-hyperparameters">Parameters vs. hyperparameters</h3>
</span>




<span class='fragment ' ><ul>
<li>What type of mask (kernel) will we use?</li>
<li>What is the best dimension?</li>
<li>How many filters will be employed?</li>
<li>What is the value of the stride?</li>
</ul>
</span>


</section><section>
<h2 id="number-of-parameters">Number of Parameters</h2>
<ul>
<li>Values in the masks that can be interpreted as <em>weights</em>: learned by the CNN during its training process.</li>
</ul>
<img width="450" src="CNN_Fig7.png" />
<p><strong>Number of parameters to be learned: 2,515.</strong></p>
</section><section>
<h3 id="what-is-the-role-of-the-hyperparameter-values-in-a-cnn">What is the role of the hyperparameter values in a CNN?</h3>
<ul>
<li><em>Kernel size:</em>
<ul>
<li><strong>Small kernels</strong> extract more local information (local features) with size reductions between layers smaller (deeper architectures)</li>
<li><strong>Larger kernels</strong>  faster size reductions of feature maps and extract more global information</li>
</ul>
</li>
<li><em>Stride value</em>: similar impact to kernel size
<ul>
<li><strong>Larger values</strong>: faster reductions of feature maps</li>
<li><strong>Smaller stride</strong>: more features being learned</li>
</ul>
</li>
</ul>
<p>

<span class='fragment ' >Since smaller stride values and kernel sizes enable more features to be learned, why not always adopt them?</span>

 

<span class='fragment ' ><strong>This requires larger datasets.</strong></span>

</p>
</section><section>
<h2 id="convolutions">Convolutions</h2>
<h4 id="in-summary-we-have-to-make-decisions-about-the-following-items-regarding-a-convolution-layer">In summary, we have to make decisions about the following items regarding a convolution layer:</h4>
<ol>


<span class='fragment ' ><li><em>Type of padding</em></li></span>




<span class='fragment ' ><li><em>Kernel size</em></li></span>




<span class='fragment ' ><li><em>Stride value</em></li></span>




<span class='fragment ' ><li><em>Number of filters</em></li></span>


</ol>
</section><section>
<h2 id="pooling">Pooling</h2>
<ul>
<li><strong>Decrease the resolution</strong> (downsampling) of the feature maps and</li>
<li>Add <strong>invariance properties</strong> to the network.</li>
<li>Feature maps size reduction leads to a decrease in the number of parameters to be learned by the network
<ul>
<li><em>More efficient training</em></li>
</ul>
</li>
<li><em>Max-Pooling</em></li>
<li><em>Average Pooling</em></li>
<li><em>Global Pooling</em></li>
</ul>
</section><section>
<h3 id="some-illustrations-to-exemplify-the-functioning-of-the-mentioned-types-of-pooling">Some illustrations to exemplify the functioning of the mentioned types of pooling.</h3>
<ul>
<li>Max-Pooling (stride = 2)</li>
</ul>
<p><img width="450" src="CNN_Fig8.png" /></p>
</section><section>
<h3 id="some-illustrations-to-exemplify-the-functioning-of-the-mentioned-types-of-pooling-1">Some illustrations to exemplify the functioning of the mentioned types of pooling.</h3>
<ul>
<li>Average Pooling (stride = 2)</li>
</ul>
<p><img width="450" src="CNN_Fig9.png" /></p>
</section><section>
<h2 id="pooling-1">Pooling</h2>
<ul>
<li>Global Pooling
<ul>
<li>More radical in the context of downsampling</li>
<li>Reduces the entire feature map to a single value</li>
<li>In this case, we can use either max-pooling or average pooling</li>
</ul>
</li>
<li>Generally, max-pooling layers tend to provide better results
<ul>
<li>more informative to use the highest value within a window than to &ldquo;mask&rdquo; them with their average value</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="flattening">Flattening</h2>
<ul>
<li>Before sending our data to the fully connected layers, we need to &ldquo;flatten&rdquo; the <strong>tensor</strong> (data)</li>
<li>Receive input with multiple dimensions (feature maps) and the output is a one-dimensional vector</li>
</ul>
<img style="margin-right:100px;" width="250" src="CNN_Fig11.png" /> 
<img width="280" src="CNN_Fig12.png" />
</section><section>
<h2 id="fully-connected--dropout">Fully Connected + Dropout</h2>
<ul>
<li>The final part consists of adopting fully connected layers
<ul>
<li>Similar to an MLP Neural Network, with a softmax output at the end</li>
<li>It is also common to adopt a regularization technique known as <strong>Dropout</strong>, which &ldquo;removes&rdquo; neurons randomly to speed up the training process and prevent overfitting</li>
</ul>
</li>
</ul>
<img width="500" src="CNN_Fig13.png" />
</section><section>
<h2 id="keras-implementation">Keras Implementation</h2>
<pre><code class="language-python">model = Sequential([
    Input(shape=(28,28,1)),
    Conv2D(filters=2,
           kernel_size=(2,2),
           strides=(1, 1),
           padding=&quot;valid&quot;, 
           activation=&quot;relu&quot;),
    MaxPooling2D(pool_size=(2, 2), 
                 strides=None, 
                 padding=&quot;valid&quot;),
    Flatten(),
    Dense(20, activation=&quot;relu&quot;),
    Dropout(rate=0.5),
    Dense(10, activation=&quot;softmax&quot;)
])
</code></pre>
<ul>
<li>The <code>Conv2D</code> represents a convolution layer</li>
<li>The <code>MaxPooling2D</code> represents a pooling strategy</li>
<li>The <code>Flatten()</code> is responsible for linearizing the output of the feature selection</li>
</ul>
</section><section>
<h1 id="building-an-mnist-solution-using-convolutions">Building an MNIST solution using Convolutions</h1>
<p>Apply your knowledge you acquire to perform the classification in the <a href="https://www.kaggle.com/datasets/zalando-research/fashionmnist">Fashion MNIST classification problem</a>.</p>
<ul>
<li>Things to try:
<ul>
<li>Increase the number of Filters;</li>
<li>Add <em>Max Pooling</em> layers;</li>
<li>Increase the amount of convolutional layers</li>
</ul>
</li>
</ul>


<span class='fragment ' ><p><a href="https://colab.research.google.com/drive/1bDRhn3SJQMZslbqLweUY5bpr0QHpT3cv?usp=sharing" target="_blank">Convnet Notebook</a></p>
</span>


</section><section>
<h1 id="covid-19-dataset-exercise">COVID-19 Dataset exercise</h1>
<p>Apply your knowledge you acquire to change the classification solution you worked for the  <a href="https://colab.research.google.com/drive/1rXZP2XofMS3ijiOkrsJSn4dUK1zzz68t?usp=sharing">COVID-19 Chest CT dataset</a>.</p>
<ul>
<li>Things to try:
<ul>
<li>Increase the number of Filters;</li>
<li>Add <em>Max Pooling</em> layers;</li>
<li>Increase the amount of convolutional layers</li>
</ul>
</li>
</ul>
<p>Remember to not linearize the data, i.e., remove the line:</p>
<pre><code class="language-python">x_train = x_train.reshape(x_train.shape[0], IMAGE_SIZE * IMAGE_SIZE)
</code></pre>
</section>
    <section><h1 id="autoencoders">Autoencoders</h1>
</section><section>
<h3 id="autoencoders-1">Autoencoders</h3>
<h4 id="some-definitions">Some Definitions:</h4>
<ul>
<li>Unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs:
<ul>
<li>Uses $y^{(i)} = x^{(i)}$</li>
<li><em>Semi-supervised</em></li>
</ul>
</li>
<li>One (or more) hidden layers that describe a latent space encoding used to represent the input data.</li>
<li>Two parts:
<ul>
<li>Encoder function $h = f(x)$, and</li>
<li>Decoder function that produces the <em>reconstruction</em> $r = g(h)$.</li>
</ul>
</li>
<li>Designed to be <em>incapable of perfectly copying the data</em>.</li>
</ul>


<span class='fragment ' ><p><strong>Question:</strong> Why is simply learning to set $g(f(x)) = x$ everywhere not particularly useful?</p>
</span>


</section><section>
<h3 id="autoencoders-2">Autoencoders</h3>
<h4 id="some-definitions-1">Some Definitions:</h4>
<ul>
<li>Special case of feedforward networks</li>
<li>Trained with the <em>same techniques</em>, such as gradient descent with minibatch, backpropagation, etc.</li>
<li>Restricted to only approximately copying, i.e., producing data that <em>merely resembles the training data</em>.</li>
<li>Forced to prioritize only certain <em>aspects</em> of the input:
<ul>
<li>Often learns useful properties of the data, e.g., relevant features.</li>
</ul>
</li>
<li>Traditionally used for <em>dimensionality reduction</em> or <em>feature learning</em>.</li>
</ul>
</section><section>
<h3 id="autoencoders-3">Autoencoders</h3>
<h4 id="example">Example:</h4>
<p align="justify">
Suppose the inputs $x$ are pixel intensity values of a <em>10×10 image</em> (100 pixels) - $n = 100$, and there are $s_2 = 50$ hidden units in layer $L_2$.
</p>
<p align="justify">
From the definition of Autoencoders, we have $y \in \mathcal{R}^{100}$. Since there are only 50 hidden units, the network is forced to learn a "compressed" representation of the input.
</p>
<p align="justify">
Given only the activation vector of hidden units $a^{(2)} \in \mathcal{R}^{50}$, it must attempt to "reconstruct" the 100-pixel input $x$.
</p>
</section><section>
<h3 id="autoencoders-4">Autoencoders</h3>
<h4 id="some-definitions-2">Some Definitions:</h4>
<ul>
<li>If there is any underlying structure in the data, such as some of the input attributes being correlated, then this algorithm <em>will be able to discover some of these correlations</em>.</li>
<li>This simple form of autoencoder will likely learn a low-dimensional representation very similar to PCA</li>
<li>Can be thought of as a <em>data compression algorithm</em></li>
<li>The compression and decompression functions are:
<ol>
<li>Data-specific,</li>
<li>Lossy, and</li>
<li>Automatically learned from examples rather than designed by us.</li>
</ol>
</li>
</ul>
</section><section>
<h3 id="autoencoders-5">Autoencoders</h3>
<h4 id="some-definitions-3">Some Definitions:</h4>
<p><img src="pca-vs-ae.png" alt="PCA vs. Autoencoder"></p>
</section><section>
<h3 id="autoencoders-6">Autoencoders</h3>
<h4 id="questions">Questions:</h4>
<ol>
<li>Why don't Autoencoders represent optimal compression algorithms <em>for general applications</em>?</li>


<span class='fragment ' ><li>Why do they <em>need to be lossy</em>?</li>
</span>




<span class='fragment ' ><li>Would an autoencoder trained on face images do a good job compressing tree images?</li>
</span>


</section><section>
<h3 id="autoencoders-7">Autoencoders</h3>
<h4 id="so-what-are-they-good-for">So, What Are They Good For?</h4>
<ul>
<li><em>Data Compression?</em>
<ul>
<li>Almost impossible to outperform standard algorithms like JPEG, MP3, etc.;</li>
<li>We can <strong>improve performance by restricting the type of data</strong> it uses;
<ul>
<li><strong>This reduces generalization capability.</strong></li>
</ul>
</li>
<li>Generally impractical for real-world data compression/compaction problems:
<ul>
<li>Can only be used on data similar to what it was trained on.</li>
</ul>
</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="autoencoders-8">Autoencoders</h3>
<h4 id="so-what-are-they-good-for-1">So, What Are They Good For?</h4>
<ul>
<li><em>Dimensionality Reduction:</em>
<ul>
<li>If the decoder is linear and the cost function is MSE, an Autoencoder learns to span the same subspace as PCA.</li>
</ul>
</li>
<li><em>Denoising:</em>
<ul>
<li>The data is partially corrupted by noise;</li>
<li>The model is trained to predict the original, non-corrupted data as its output.</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="autoencoders-9">Autoencoders</h3>
<h4 id="implementation">Implementation</h4>
<p>General Architecture:</p>
<img src="autoencoder1.png" width="700px">
<p><em>Source: <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Autoencoder architecture by Lilian Weng</a></em></p>
</section><section>
<h3 id="autoencoders-10">Autoencoders</h3>
<h4 id="implementation-1">Implementation:</h4>
<p>Dimensionality reduction for 3D data:</p>
<pre><code class="language-python">encoding_dim = 2
input_layer = keras.Input(shape=(3,))
encoded = layers.Dense(encoding_dim, activation=&quot;sigmoid&quot;)(input_layer)
decoded = layers.Dense(3, activation=&quot;sigmoid&quot;)(encoded)
autoencoder = keras.Model(input_layer, decoded)
autoencoder.compile(loss=&quot;mse&quot;, optimizer=&quot;SGD&quot;)
</code></pre>
</section><section>
<h3 id="autoencoders-11">Autoencoders</h3>
<h4 id="implementation-2">Implementation</h4>
<p><img src="autoencoder2.png" alt="Autoencoder architecture"></p>
</section><section>
<h3 id="autoencoders-12">Autoencoders</h3>
<h4 id="implementation-3">Implementation</h4>
<img src="autoencoder3.png" width="300px">
<img src="autoencoder4.png" width="300px">
</section><section>
<h3 id="autoencoders-13">Autoencoders</h3>
<h4 id="what-are-they-not-good-at">What Are They Not Good At?</h4>
<img src="autoencoder5.png" width="500px">
</section><section>
<h3 id="autoencoders-14">Autoencoders</h3>
<h4 id="what-are-they-not-good-at-1">What Are They Not Good At?</h4>
<ul>
<li>Some of the biggest challenges regarding the latent space are:
<ul>
<li><em>Gaps in the latent space</em>: we don&rsquo;t know what data points might look like in those spaces.</li>
<li><em>Separability in the latent space</em>: there are also regions where the labels are intermixed/randomly scattered.</li>
<li><em>Discrete latent space</em>: we don&rsquo;t have a trained statistical model for an arbitrary input.</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="autoencoders-15">Autoencoders</h3>
<h4 id="limitations">Limitations</h4>
<ul>
<li>Unfortunately, autoencoders <strong>do not learn anything useful</strong> if the encoder and decoder <strong>have too much capacity</strong>.</li>
<li>This also happens if the latent space has the same dimension as the input.</li>
<li>Even a linear encoder and decoder can <em>learn to copy the input</em> to the output:
<ul>
<li><em>Nothing useful is learned about the data distribution.</em></li>
</ul>
</li>
</ul>
</section><section>
<h3 id="regularized-autoencoders">Regularized Autoencoders</h3>
<h4 id="denoising-autoencoders">Denoising Autoencoders</h4>
<ul>
<li>The input is <em>partially corrupted</em> by adding noise or &ldquo;masking&rdquo; some values of the input vector stochastically;</li>
<li>The model is trained to <em>recover the original input</em> (note: not the corrupted one);</li>
</ul>
<img src="denoising_autoencoder.png" width="400px">
<p>where $\mathcal{M_D}$ defines the mapping of true data samples to noisy or corrupted ones.</p>
</section><section>
<h3 id="regularized-autoencoders-1">Regularized Autoencoders</h3>
<h4 id="denoising-autoencoders-1">Denoising Autoencoders</h4>
<p><img src="autoencoder7.png" alt="Denoising Autoencoder"></p>
<p><em>Source: Image by Lilian Weng</em></p>
</section><section>
<h3 id="regularized-autoencoders-2">Regularized Autoencoders</h3>
<h4 id="denoising-autoencoders-2">Denoising Autoencoders</h4>
<ul>
<li>Motivated by the fact that humans can easily recognize an object even with partially occluded vision;</li>
<li>To &ldquo;repair&rdquo; the input, the DAE must discover the <em>relationship between the input dimensions to infer the missing parts</em>;</li>
<li>In images, the model is likely to rely on evidence gathered from a <em>combination of many input dimensions</em> to recover the noise-free version:
<ul>
<li>This creates a good foundation for learning a robust latent representation;</li>
</ul>
</li>
<li>In the <a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390294">original DAE paper</a>, a fixed proportion of input dimensions is randomly selected, and their values are forced to 0 (similar to dropout?).</li>
</ul>
</section><section>
<h3 id="regularized-autoencoders-3">Regularized Autoencoders</h3>
<h4 id="denoising-autoencoders---implementation">Denoising Autoencoders - Implementation</h4>
<p>Example for the MNIST dataset:</p>
<pre><code class="language-python">noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 
</code></pre>
<p><img src="autoencoder8.png" alt="Digits from MNIST after adding noise"></p>
<p><a href="https://colab.research.google.com/drive/1KzErreZmln2zvDnEzXPIOqwpo2-8pgZ7?usp=sharing"><em>Jupyter Notebook</em></a> with the example implementation of Denoising Autoencoder for the MNIST.</p>
</section><section>
<h1 id="variational-autoencoders">Variational Autoencoders</h1>
</section><section>
<h3 id="introduction">Introduction</h3>
<h3 id="recap-the-problem-with-the-standard-autoencoder">Recap: The problem with the standard autoencoder</h3>
<font size="5">
<ul>
<li>Besides some efficient applications like denoising autoencoders, they are quite limited.</li>
<li>The latent space to which they convert their inputs, and where their encoding vectors reside, may not be continuous or allow for easy interpolation.</li>
<li>For example, training an autoencoder on the MNIST dataset and visualizing the encodings in a 2D latent space reveals the formation of distinct clusters:
</font></li>
</ul>
<img src="vae1.png" width="300px">
</section><section>
<h3 id="variational-autoencoders-1">Variational Autoencoders</h3>
<h4 id="recap-the-problem-with-the-standard-autoencoder-1">Recap: The problem with the standard autoencoder</h4>
<ul>
<li>When building a generative model, we don&rsquo;t want to replicate the input data:
<ul>
<li>Randomly sample from the latent space, or</li>
<li>Generate variations in an input image from a continuous latent space;</li>
<li>If the space has discontinuities and you sample/generate a variation from there, the decoder will simply produce an unrealistic output;</li>
<li>The decoder has no idea how to handle that region of the latent space;</li>
<li>During training, it never saw encoded vectors coming from that region of the latent space;</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="variational-autoencoders-2">Variational Autoencoders</h3>
<h4 id="definitions">Definitions</h4>
<ul>
<li>Variational Autoencoders (VAEs) have a fundamentally unique property that separates them from common autoencoders:
<ul>
<li>Their latent spaces are inherently continuous;</li>
<li>The continuity of the latent space allows for easy random sampling and interpolation.</li>
</ul>
</li>
<li>Their encoder does not produce a coding vector of size $n$;</li>
<li>Instead, it generates two vectors of size $n$:
<ul>
<li>A vector of means, $\mu$, and</li>
<li>Another vector of standard deviations, $\sigma$.</li>
<li>The mean and the standard deviation of the $i$-th random variable, $X_i$, from which we sample to obtain the sampled encoding passed to the decoder;</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="variational-autoencoders-3">Variational Autoencoders</h3>
<h4 id="definitions-1">Definitions</h4>
<p><img src="vae2.png" alt="Variational Autoencoder with $\mu$ and $\sigma$ vectors">
<em>Source: <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf" target=_blank>Variational Autoencoder architecture by Irhum Shafkat</a></em></p>
</section><section>
<h3 id="variational-autoencoders-4">Variational Autoencoders</h3>
<h4 id="example-1">Example</h4>
<p>In a scenario where we have an input signal with 500 features and we intend to reduce this signal to just 30, we could build a VAE as follows:</p>
<img src ="vae3.png" width="300px">
</section><section>
<h3 id="variational-autoencoders-5">Variational Autoencoders</h3>
<h4 id="definitions-2">Definitions</h4>
<img src="vae4.png" width="500px">
</section><section>
<h3 id="variational-autoencoders-6">Variational Autoencoders</h3>
<h4 id="definitions-3">Definitions</h4>
<font size="5">
<ul>
<li>Stochastic generation of encoding vectors.
<ul>
<li>For the same input, keeping the mean and standard deviation the same, the actual encoding will vary on each pass due to sampling.</li>
</ul>
</li>
<li>The mean vector controls where the encoding of an input should be centered;</li>
<li>The standard deviation controls how much the encoding can vary from the mean (the area).</li>
</ul>
</font>
<img src="vae5.png" width="250px">
</section><section>
<h3 id="variational-autoencoders-7">Variational Autoencoders</h3>
<h4 id="definitions-4">Definitions</h4>
<ul>
<li>Not just a single point in the latent space refers to a sample of that class.</li>
<li>All nearby points refer to the same within a $\sigma$ radius;</li>
<li>The goal here is to create a more homogeneous latent space, eliminating discontinuity;
<ul>
<li>The model is now exposed to a certain degree of local variation by varying the encoding of a sample;</li>
<li>We want overlap between samples that are also not very similar;
<ul>
<li>Interpolation between classes;</li>
</ul>
</li>
</ul>
</li>
</ul>
</section><section>
<h3 id="variational-autoencoders-8">Variational Autoencoders</h3>
<h4 id="definitions-5">Definitions</h4>
<ul>
<li>There are no limits to the values that the $\mu$ and $\sigma$ vectors can assume:
<ul>
<li>The encoder can learn to generate very different $\mu$ values for different classes, clustering them and minimizing $\sigma$;</li>
<li>It can reach a point that appears as a single point.</li>
</ul>
</li>
<li>Desirable: Encodings that are as close as possible while still distinct, allowing for smooth interpolation and the possibility of constructing new samples.</li>
</ul>
</section><section>
<h3 id="variational-autoencoders-9">Variational Autoencoders</h3>
<h4 id="definitions-6">Definitions</h4>
<p>What we want and what we can achieve:</p>
<p><img src="vae6.png" alt="Desired latent space"></p>
</section><section>
<h3 id="variational-autoencoders-10">Variational Autoencoders</h3>
<h4 id="definitions---the-kl-divergence">Definitions - The KL Divergence</h4>
<font size="5">
<ul>
<li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler divergence</a></li>
<li>Measures how much they diverge from each other;</li>
<li>For VAEs, the cost by KL is equivalent to the sum of all KL divergences between the component $X_i\sim\mathcal{N}(\mu_i, \sigma_i^2)$ and the standard normal distribution.
<ul>
<li>This measure is minimized when $\mu_i=0$ and $\sigma_i=0$;</li>
</ul>
</li>
<li>When the divergence is calculated between univariate distributions, it can be simplified to [1]:
$$
\sum_{i=1}^n \sigma_i^2+\mu_i^2 - \log(\sigma_i^2)-1
$$</li>
</ul>
<p><em>Source: <a href="https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes/370048#370048">Deriving the KL divergence loss for VAEs</a></em></p>
</font>
</section><section>
<h3 id="variational-autoencoders-11">Variational Autoencoders</h3>
<h4 id="definitions---the-kl-divergence-1">Definitions - The KL Divergence</h4>
<font size="5">
<ul>
<li>Forces the encoder to distribute all encodings uniformly around the center of the latent space;</li>
<li>Using purely the result of the KL loss results in a latent space with densely placed encodings randomly, near the center of the latent space;</li>
<li>The decoder finds it impossible to decode anything meaningful from this space;</li>
</ul>
</font>
<img src="vae7.png" width="250px">
</section><section>
<h3 id="variational-autoencoders-12">Variational Autoencoders</h3>
<h4 id="grouping-the-information">Grouping the information&hellip;</h4>
<font size="5">
<ul>
<li>Use the KL divergence as a penalization mechanism;</li>
<li>Optimize the composite loss (e.g., reconstruction, or cross-entropy) and the KL divergence;
<ul>
<li>Generate a latent space that maintains the similarity of nearby encodings;</li>
<li>Globally, it is densely packed near the origin of the latent space;</li>
<li>Balance is achieved by the clustering nature of the reconstruction loss and the dense packing nature of the KL loss;</li>
</ul>
</li>
</ul>
</font>
<img src="vae8.png" width="250px">
</section>

</div>
      
<div class="line top"></div>
<div class="line bottom"></div>



    </div>
<script type="text/javascript" src=./reveal-hugo/object-assign.js></script>

<a href="./reveal-js/css/print/" id="print-location" style="display: none;"></a>
<script type="text/javascript">
  var printLocationElement = document.getElementById('print-location');
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = printLocationElement.href + (window.location.search.match(/print-pdf/gi) ? 'pdf.css' : 'paper.css');
  document.getElementsByTagName('head')[0].appendChild(link);
</script>

<script type="application/json" id="reveal-hugo-site-params">{"history":true,"templates":{"grey":{"background":"#424242","transition":"convex"}}}</script>
<script type="application/json" id="reveal-hugo-page-params">{"custom_theme":"reveal-hugo/themes/robot-lung.css","highlight_theme":"mono-blue","margin":0,"templates":{"hotpink":{}},"transition":"convex","transition_speed":"fast"}</script>

<script src="./reveal-js/js/reveal.js"></script>

<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }
  
  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams));
  Reveal.initialize(options);
</script>


  
  
  <script type="text/javascript" src="./reveal-js/plugin/markdown/marked.js"></script>
  
  <script type="text/javascript" src="./reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="./reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="./reveal-js/plugin/zoom-js/zoom.js"></script>
  
  
  <script type="text/javascript" src="./reveal-js/plugin/notes/notes.js"></script>























  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({startOnLoad: false});
    let render = (event) => {
      let mermaidElems = event.currentSlide.querySelectorAll('.mermaid');
      if (!mermaidElems.length){
          return
      }
      mermaidElems.forEach(mermaidElem => {
          let processed = mermaidElem.getAttribute('data-processed');
          if (!processed){
              
              mermaid.init(undefined, mermaidElem);
          }
      });
    };
    Reveal.addEventListener('slidechanged', render);
    Reveal.addEventListener('ready', render);
  </script>

    
    


<script type="text/javascript">

Reveal.addEventListener('slidechanged', function(event) {
  console.log("🎞️ Slide is now " + event.indexh);
});
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<style>
  #logo {
    position: absolute;
    top: 30px;
    left: 75%;
    width: 300px;
  }
</style>
<img id="logo" src="logo_ai2.png" alt="Advanced Institute for Artificial Intelligence">

  </body>
</html>
